{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, dim_init=6075, dim_middle=1024, dim_latent=100):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(dim_init, dim_middle),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.latent_mu = nn.Linear(dim_middle, dim_latent)\n",
    "        self.latent_logsigma = nn.Linear(dim_middle, dim_latent)\n",
    "        \n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(dim_latent, dim_middle),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.reconstruction_mu = nn.Sequential(\n",
    "            nn.Linear(dim_middle, dim_init),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.reconstruction_logsigma = nn.Sequential(\n",
    "            nn.Linear(dim_middle, dim_init),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            std = logsigma.exp()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_enc = self.encode(x)\n",
    "        latent_mu = self.latent_mu(x_enc)\n",
    "        latent_logsigma = self.latent_logsigma(x_enc)\n",
    "        \n",
    "        z = self.gaussian_sampler(latent_mu, latent_logsigma)\n",
    "        \n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        reconstruction_mu = self.reconstruction_mu(x_hat)\n",
    "        reconstruction_logsigma = self.reconstruction_logsigma(x_hat)\n",
    "        \n",
    "        return reconstruction_mu, reconstruction_logsigma, latent_mu, latent_logsigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(mu, logsigma):\n",
    "    return - 0.5 * torch.sum(1 + 2 * logsigma - mu.pow(2) - logsigma.exp().pow(2), dim=1)\n",
    "\n",
    "def log_likelihood(x, mu, logsigma):\n",
    "    return torch.sum(- logsigma - 0.5 * np.log(2 * np.pi) - (mu - x).pow(2) / (2 * logsigma.exp().pow(2)), dim=1)\n",
    "\n",
    "def loss_beta_vae(x, mu_gen, logsigma_gen, mu_latent, logsigma_latent, beta=1):\n",
    "    return torch.mean(beta * KL_divergence(mu_latent, logsigma_latent) - log_likelihood(x, mu_gen, logsigma_gen))\n",
    "\n",
    "def reconstruction_error():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train $\\beta$-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "def train(model, opt, scheduler, loss_beta_vae, train_loader, valid_loader, num_epochs=20, beta=1):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    train_mean_loss = []\n",
    "    valid_mean_loss = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # a full pass over the training data:\n",
    "        start_time = time.time()\n",
    "        model.train(True)\n",
    "        for (X_batch, y_batch) in train_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                reconstruction_mu, reconstruction_logsigma, latent_mu, latent_logsigma = \\\n",
    "                model.forward(Variable(X_batch).cuda())\n",
    "                loss = loss_beta_vae(Variable(torch.FloatTensor(X_batch)).cuda(), \\\n",
    "                                     reconstruction_mu, reconstruction_logsigma, latent_mu, latent_logsigma, beta)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                train_loss.append(loss.data.cpu().numpy()[0])\n",
    "            else:\n",
    "                reconstruction_mu, reconstruction_logsigma, latent_mu, latent_logsigma = \\\n",
    "                model.forward(Variable(X_batch))\n",
    "                loss = loss_beta_vae(Variable(torch.FloatTensor(X_batch)), \\\n",
    "                                     reconstruction_mu, reconstruction_logsigma, latent_mu, latent_logsigma, beta)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                train_loss.append(loss.data.numpy()[0])\n",
    "\n",
    "        # a full pass over the validation data:\n",
    "        model.train(False)\n",
    "        for (X_batch, y_batch) in valid_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                reconstruction_mu, reconstruction_logsigma, latent_mu, latent_logsigma = \\\n",
    "                model.forward(Variable(X_batch).cuda())\n",
    "                loss = loss_beta_vae(Variable(torch.FloatTensor(X_batch)).cuda(), reconstruction_mu, \\\n",
    "                                     reconstruction_logsigma, latent_mu, latent_logsigma, beta)\n",
    "                valid_loss.append(loss.data.cpu().numpy()[0])\n",
    "            else:\n",
    "                reconstruction_mu, reconstruction_logsigma, latent_mu, latent_logsigma = \\\n",
    "                model.forward(Variable(X_batch))\n",
    "                loss = loss_beta_vae(Variable(torch.FloatTensor(X_batch)), reconstruction_mu, \\\n",
    "                                     reconstruction_logsigma, latent_mu, latent_logsigma, beta)\n",
    "                valid_loss.append(loss.data.numpy()[0])\n",
    "                \n",
    "        train_mean_loss.append(np.mean(train_loss[-len(train_loader) // train_loader.batch_size :]))\n",
    "        valid_mean_loss.append(np.mean(valid_loss[-len(valid_loader) // valid_loader.batch_size :]))\n",
    "        \n",
    "        # update lr\n",
    "        scheduler.step(valid_mean_loss[-1])\n",
    "        # stop\n",
    "        if opt.param_groups[0]['lr'] <= 1e-6:\n",
    "            break\n",
    "        \n",
    "        # visualization of training\n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"#epoch\")\n",
    "        plt.ylabel(\"losses\")\n",
    "        plt.plot(train_mean_loss, 'b', label='Training loss')\n",
    "        plt.plot(valid_mean_loss, 'r', label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            train_mean_loss[-1]))\n",
    "        print(\"  validation loss (in-iteration): \\t{:.6f}\".format(\n",
    "            valid_mean_loss[-1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    vae_model = vae_model.cuda()\n",
    "\n",
    "# optimizer = optim.Adam(vae_model.parameters(), lr=1e-2)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
